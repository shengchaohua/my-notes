[TOC]

# 卷积神经网络
卷积，是指局部链接，权值共享。每层神经元只和上一层的部分神经元相连，且过滤器的权值对于上一层所有神经元都是一样的。

## 经典卷积神经网络
LeNet是最早的神经网络之一，用于解决手写数字识别的问题。LeNet奠定了现代卷积神经网络的原型，即卷积，非线性映射，池化，全连接。

AlexNet继承了LeNet的基本结构，但是网络更大。AlexNet的特点包括：1）采用relu替代了tanh和sigmoid激活函数；2）全连接层使用了dropout来防止过拟合；3）使用了数据增强技术。

VGG16是VGG模型的一种。VGG采用了比较深的、比较整齐的网络，卷积核大小都是3×3，最大池化都是2×2，使得卷积前后大小不变，池化后缩小一半。串联两个3×3卷积相当于一个5×5卷积，减少了参数。VGG有11层、13层、16层、19层等不同的结构，证明了网络越深，提取的高阶特征越多，从而准确率得到提升。

GoogLeNet网络在深度和宽度上同时进行了扩展，并且先后发展了四个版本。它引入了inception模块，其中同时使用了1×1、3×3、5×5卷积。通过对不同卷积核的卷积结果进行叠加，减少了参数数量。由于最后的全连接层容易出现过拟合，使用了全局平均池化层来替代。

ResNet引入了跳跃连接，把前面某层的输出作为后面某层的输入，这一部分称为残差模块。前面层的输出加到后面层的激活，这里假设二者形状相同。如果不相同，可以用一个线性变换。ResNet解决了梯度消失和表示瓶颈两个重要问题，使得网络能够非常深。


## 批量归一化（Batch Norm）
> [5.10. 批量归一化 - 动手学深度学习](https://zh.d2l.ai/chapter_convolutional-neural-networks/batch-norm.html)

神经网络在训练的过程中，层之间的数据分布会发生变化，参数更新的过程中容易落到激活函数的饱和区，梯度变化比较小，导致梯度更新较慢。

批量归一化利用小批量上的**均值**和**标准差**，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定，加快梯度更新。批量归一化层置于全连接层中的**仿射变换**和**激活函数**之间。

批量归一化会导致一些参数位于0附近，函数接近线性，非线性表达的能力被削弱了，所以需要进行拉伸和偏移操作。

![批量归一化](https://note.youdao.com/yws/api/personal/file/E0BB7F01A2CD48E69FF13816490D32DF?method=download&shareKey=2e07fe0b899b6dd17b2c87bccd5c85ed)

值得注意的是，可学习的拉伸和偏移参数保留了不对 `$x(i)$` 做批量归一化的可能：此时只需学出 `$\gamma=\sqrt{\sigma_{\beta}^2+\epsilon}$` 和 `$\beta=\mu_{\beta}$` 。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。

优点：
1. 使得每层输入数据的分布相对稳定，改善网络的梯度，减少梯度消失。
2. 可以使用较大的学习率，加快网络收敛速度；
3. 破坏原来的数据分布，减少对网络初始化的依赖，有一定正则化效果，缓解过拟合；

如何对卷积层做批量归一化？对于一批数据，如果卷积计算输出多个通道，我们需要对每个通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。

预测时如何做批量归一化？将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于单个小批量的均值和方差。一种常用的方法是通过移动平均估算训练过程中所有批次的样本均值和方差，并在预测时使用它们得到确定的输出。


## 梯度消失
梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，网络参数不再变化。

梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层。如果计算得到的梯度值非常小，随着层数增多，梯度将会以指数形式衰减，就会发生梯度消失。


##  梯度爆炸
在深度网络或循环神经网络（ RNN）等网络结构中，梯度可在网络更新的过
程中不断累积，变成非常大的梯度，导致网络参数的大幅更新，使得网络不稳定；在极端情况下，参数甚至会溢出，变为NaN值，再也无法更新。

## dropout有什么缺点？
dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制代价函数的变化图像。为了保证正确，可以先关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入其他问题。

## 1×1卷积
Network in Network是第一篇探索1×1卷积核的论文。这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力。1×1卷积核让不同通道的特征能够交互整合，使通道之间的信息得以流通。

GoogLeNet使用1×1卷积来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了多个的卷积核，大大增加了模型的参数量。如果在较大卷积核的卷积层前引入1×1卷积，可以对卷积核通道数进行降维，减少参数量。

