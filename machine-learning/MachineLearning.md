[TOC]








# 逻辑回归
## 介绍
逻辑回归本质是一个线性模型，只需要求出超平面的参数。逻辑回归的输出值被看作属于正类的概率。模型简单，可解释性强，而且可以使用分布式实现。

## 推导
一个事件的几率$odds$是指该事件发生的概率与该事件不发生的概率的比值。

给定一个测试样本，逻辑回归把它属于正类的概率$p$和属于负类的概率$1-p$的比值看作一个几率，并把几率的对数看作一个线性函数。推导过程为最大化似然函数，也可以解释为最小化损失函数。

$$
odds = \frac{p}{1-p} \\
\log{odds} = w^Tx \\
p = \frac{1}{1+e^{-w^Tx}} \\
h_{w}(x) = p \\
\prod_{i=1}^{N}(h_w(x_i))^{y_i}(1-h_w(x_i))^{1-y_i} \\
L = -\frac{1}{N}\sum_{i=1}^{N}[y_i \log{h_w(x_i)} + (1-y_i)\log{(1-h_w(x_i)}] \\
\frac{\partial{L}}{\partial{w}} = \frac{1}{N}\sum_{i=1}^{N}(h_w(x_i) - y_i) x_{i}
$$

## 损失函数
逻辑回归使用的损失函数是二分类交叉熵损失函数：$-(y\log h(x)+(1-y)\log(1-h(x)))$，其中y=0或1，$h(x)=1/(1+e^{-wx})$为sigmoid函数，其输出值可以认为是该样本属于正类的概率。



# FM算法

> [Factorization Machines 学习笔记（一）预测任务](http://blog.csdn.net/itplus/article/details/40534885)

> [FM算法(一)：算法理论](https://www.cnblogs.com/AndyJee/p/7879765.html)

> [FM系列算法解读（FM+FFM+DeepFM）](https://blog.csdn.net/hiwallace/article/details/81333604)

> [CTR预估模型FM、FFM、DeepFM](https://www.biaodianfu.com/ctr-fm-ffm-deepfm.html)


## 介绍

FM（Factorization Machine）主要是为了解决数据稀疏的情况下，特征组合的问题。其主要优点包括：可用于高度稀疏数据场景；具有线性的计算复杂度。

## 描述

在数据稀疏的场景中，比如广告点击率预测，数据经过onehot处理之后会比较稀疏。对特征进行组合，但是特征同时为1的情况比较少，导致学习困难。

为每个特征分量引入一个$k$维的辅助向量，本质是对权重矩阵$W$进行分解：当$k$足够大时，对于任意对称正定的实矩阵$W\in R^{n \times n}$，均存在实矩阵$V \in R^{n \times k}$，使得$W=VV^T$成立。

理论分析中，我们要求参数$k$取得足够大。但是，在高度稀疏的场景中，由于没有足够的样本来估计复杂的交互矩阵，因此$k$通常取得比较小。

==TODO==



# 支持向量机

## 介绍
支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：
- 当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；
- 当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；
- 当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；

## 推导
1、定义函数间隔$\hat{r_i}=y_i(wx_i+b)$和几何间隔$r_i=\frac{y_i(wx_i+b)}{||w||}$。

2、得到优化目标，化为最简形式。
$$
max\; r \;\;\; s.t.\; \frac{y_i(wx_i+b)}{||w||} \ge r \\
min\; \frac{1}{2}||w|| \;\;\; s.t.\; y_i(wx_i+b) \ge 1
$$

3、引入拉格朗日乘子$\alpha_i$。求解最小最大问题$\underset{w,b}{min}\;\underset{\alpha}{max}\;L(w,b,\alpha)$和对偶问题$\underset{\alpha}{max}\;\underset{w,b}{min}\;L(w,b,\alpha)$。
$$
L(w,b,\alpha)=\frac{1}{2}w^{T}w-\sum_{i=1}^{m}(y_i(wx_i+b)-1) \\
\underset{w,b}{min}\;\underset{\alpha}{max}\;L(w,b,\alpha) \Leftrightarrow \underset{\alpha}{max}\;\underset{w,b}{min}\;L(w,b,\alpha)
$$



## 损失函数

SVM使用的损失函数是合页损失：$[1-y(w \cdot x +b)]_+$。

## 为什么引入对偶问题？
1. 对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。
2. 可以很自然的使用核函数，进而推广到非线性的情况。

## 核函数
引入核函数目的是把原坐标系里线性不可分的数据用核函数投影到另一个空间，尽量使得数据在新的空间里线性可分。

常见的核函数：线性核、多项式核、指数核、高斯核等。

## SVM的主要特点和缺点
特点：
- SVM的目标是对特征空间划分得到最优超平面。
- SVM的理论基础是非线性映射，可以利用核函数代替向高维空间的非线性映射。
- SVM的分类超平面只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目。
- SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。
- SVM适用于小样本数据集上，能够得到比较好的结果。

缺点：
- SVM对大规模训练样本难以实施。SVM的空间消耗主要是存储训练样本和核矩阵，当样本数目很大时将耗费大量的机器内存和运算时间。
- 用SVM解决多分类问题存在困难。需要转化为一对多或一对一分类问题。
- 对缺失数据敏感，对参数和核函数的选择敏感。


## SVM和LR的相同点和不同点
相同点：
1. LR和SVM都是分类算法
2. LR和SVM都是监督学习算法
3. LR和SVM都是判别模型
4. 如果不考虑核函数，LR和SVM都是线性分类算法，因为分类决策面都是线性的

不同点：
1. LR采用对数（log）损失，SVM采用合页（hinge）损失。
2. 逻辑回归基于概率理论，认为样本为正类的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。支持向量机基于几何边界最大化原理。
3. LR对异常值敏感，SVM对异常值不敏感。支持向量机只考虑局部的超平面附近的点，而逻辑回归考虑全局。支持向量机改变非支持向量样本并不会引起决策面的变化，逻辑回归中改变任何样本都会引起决策面的变化。
4. 计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高。
5. 对非线性问题的处理方式不同。LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数。
6. SVM的损失函数就自带正则化，LR需要额外添加正则化。所以，SVM是结构风险最小化，LR是经验风险最小化。




# 决策树
## 介绍
决策树是一种基于规则的算法。决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。

决策树学习算法通常是递归地选择最优特征，并根据该特征对训练数据进行分割。这一过程对应着特征空间的划分，也对应着决策树的构建。

决策树学习算法主要包括三个过程：特征选择，决策树生成和决策树剪枝。


## 具体算法
具体的决策树算法有：ID3，C4.5，CART等。ID3和C4.5可以是多叉决策树，CART规定是二叉决策树。

ID3使用最大化信息增益作为特征选择的标准。熵是表示随机变量不确定性的度量。熵的范围是0-1，熵越大表示随机变量越不确定。熵可以用来衡量数据集的不确定性，数据集中如果都是一个类别的数据，则不确定性最低。

> 信息增益=分割前数据集的熵-分割后多个子数据集的加权熵之和。

C4.5使用最大化信息增益率作为特征选择的标准。

> 信息增益率=信息增益/分割前数据集的熵。

CART回归树使用最小化平方误差作为特征选择的标准。

CART分类树使用最小化Gini指数作为特征选择的标准。Gini指数可以用来衡量一个离散型概率分布的不确定性，所以可以根据样本的类标签衡量一个样本集合的不确定性。基尼指数越大，样本集合的不确定性也就越大。

> 根据一个特征分割数据集的Gini指数=根据该特征分割后两个的子数据集的加权Gini指数之和。


## 剪枝
决策树剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。

- 预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能，如果不能提升，则停止划分，将当前节点标记为叶结点。
- 后剪枝：生成决策树以后，再自下而上对非叶结点进行考察，若将此节点标记为叶结点可以带来泛化性能提升，则修改之。



# 集成学习

> [【机器学习】模型融合方法概述](https://zhuanlan.zhihu.com/p/25836678)

> [【机器学习】集成学习之stacking](https://blog.csdn.net/qq_32742009/article/details/81366768)

## 介绍
集成学习（ensemble learning）通过构建并结合多个弱学习器（也叫基学习器）来完成机器学习任务，提升单个学习器的性能。

集成学习主要三个类别：Bagging，Boosting和Stacking。Bagging有随机森林算法，Boosting有Adaboost、GBDT等算法，Stacking没有具体的算法，只是把多种算法融合在一起。

Bagging使用了有放回抽样方法，每次从训练数据抽取N个样本（N是训练数据的大小），训练一个基学习器，最后综合多个基学习器得到最终的模型。Bagging中的基学习器可以并行训练，彼此之间没有依赖关系。

Boosting也可以称为提升方法，实际采用加法模型（基学习器的线性组合）与前向分步算法。Boosting会初始化一个先验模型作为当前模型。根据训练数据的权值或者根据训练数据和当前模型的残差训练一个基学习器，并将这个基学习器添加到当前模型中，下一轮会根据训练数据和更新后的模型再训练一个基学习器。所以，Boosting中的基学习器是相互依赖的，只能串行训练。

Stacking是一种可以把多种算法融合在一起的集成方法，也称为模型融合。给定训练集和测试集，选择一个模型，比如逻辑回归，对训练集进行`K`折交叉验证，因此可以训练`K`个逻辑回归模型。对于每个模型，
- 需要对对应的验证集进行预测，然后把所有验证集的预测结果叠加起来，可以得到一个列向量，长度等于训练集的大小。该向量可以看作训练集的一个新的特征。
- 需要对预测集进行预测，然后把测试集的预测结果相加并求平均值，长度等于测试集的大小。该向量可以看作测试集的特征。

接下来可以选择参数不同的逻辑回归模型或者选择其他模型，再生成多个新的特征。

最后，在新的训练集进行训练一个算法，并对新的测试集进行预测。

Stacking过程如下图所示：

![机器学习 集成学习 stacking](https://raw.githubusercontent.com/shengchaohua/MyImages/main/images/20201101141851.jpg?token=AE4F4YO5G5CSBKE7BCCJ2DC7TZJ6W)

## Bagging和Boosting的区别
Bagging选择不稳定的、方差大的基学习器，比如决策树和神经网络，不可以选线性模型；Boosting方法选择拟合能力强的基学习器，比如决策树和线性学习器。

从方差和偏差的角度看，Bagging方法主要关注降低方差，Boosting方法主要关注降低偏差。但是在GBDT的实现中，也可以使用数据和特征抽样方法来降低方差。

Boosting降低偏差比较容易理解，每次对当前模型在训练数据上的预测值与真实值的残差拟合一棵树，使训练误差越来越小。

Bagging降低方差不太好理解。Bagging中的每个基学习器在不完全相同的数据集上训练，基学习器之间的方差比较大。但是，基学习器的效果是优于随机的，而且每个学习器的误差有一定独立性。通过集成所有基学习器的预测结果，可以使模型的预测结果趋于正确值，降低了模型的方差，提高了模型的泛化能力。




# 随机森林
随机森林（Random Forest）是一种Bagging集成算法。它对Bagging进行了改进，选择CART决策树作为基学习器；还有在分裂结点时，随机抽取一部分特征并选择其中最好的特征。




# Adaboost
## 介绍
Adaboost是一种Boosting集成算法。它为每一个样本分配了一个权重，根据训练数据和对应的权重训练一个基学习器，计算该学习器的加权分类误差率，然后根据分类误差率调整每个样本的权重：减小分类正确的样本的权重，增大分类错误的样本的权重。Adaboost依次训练多个基学习器，最终组合所有学习器为最终的模型。

## 描述
训练数据是二类的，目标值是`-1`或`+1`。训练集大小为`N`，每个样本的初始权重为`1/N`，权重之和为`1`。

需要训练多个基学习器，对于一个基学习器：
- 根据当前的训练数据和权重，训练一个基学习器$G_m$，使该学习器的分类误差率最低。分类误差率是分类错误的数据的权重之和。
- 假设分类误差率为$e_m$，计算该基学习器的系数$\alpha_m=\frac{1}{2}ln\frac{1-e_m}{e_m}$。分类正确的样本的权重乘以$e^{-\alpha_m}$，分类错误的样本的权重乘以$e^{\alpha_m}$，然后对所有样本的权重进行归一化，保证权重之和为`1`。
- 因为分类误差率小于二分之一，所以$\alpha_m>0$，分类正确的样本权重减小，分类错误的样本权重增大。

最后，构建基学习器的线性组合：$G(x)=sign(\sum_{m=1}^{M}\alpha_mG_m(x))$。



# 提升树
## 介绍
提升树（Boosting Tree）是一种Boosting集成算法。提升树算法使用决策树作为基学习器，对分类问题是二叉分类树，回归问题是二叉回归树。

对于二分类问题，提升树算法只需要将AdaBoost 算法中的基学习器限制为二叉分类树即可。可以说这时的提升树算法是Adaboost算法的特殊情况。

对于回归问题，提升树算法每次根据**训练数据的真实目标值**和**当前模型的预测值**的**残差**训练一个基学习器，更新当前模型，并不断重复。

前提：
1. 提升树模型可表示为决策树的加法模型：$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$。
2. 首先初始化提升树 $f_0(x)=0$，则第 $m$ 步的模型为：$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$。
3. 通过经验风险最小化确定下一棵决策树的参数。换句话说，下一棵决策树需要拟合训练数据和当前模型的残差。
$$
\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^NL(y_i,{\color{Red} f_{m-1}(x_i)+T(x_i;\Theta_m)})
$$

## 描述
输入：训练集 $T=\{(x_1,y_1),..,(x_N,y_N)\},\; x_i \in R^n,\; y_i \in R$。

输出：提升树模型$f_M(x)$。

1. 初始化当前模型$f_0(x)=0$
2. 对 $m=1,2,...,m$，
    1. 计算残差，${\color{Red} r_{m,i}}=y_i-f_{m-1}(x_i),\quad i=1,2,..,N$
    2. 拟合残差，学习下一个回归树的参数，$\hat{\Theta}_m=\arg\underset{\Theta_m}{\min}\sum_{i=1}^N L({\color{Red} r_{m,i}},{\color{Blue} T(x_i;\Theta_m)})$
    3. 更新当前模型，$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$
3. 得到回归提升树，$f_M(x)=\sum_{m=1}^MT(x;\Theta_m)$



# GBDT
## 介绍
GBDT（Gradient Boosting Decision Tree，梯度提升树）是一种以提升树算法为基础的Boosting集成算法。它使用CART回归树作为基学习器，不管是分类任务还是回归任务。

GBDT的关键是**利用损失函数在当前模型的负梯度作为残差的近似值**，根据不同的任务选择不同的损失函数。

**用梯度近似的好处**：提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步的优化很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。利用负梯度近似残差，可以引入更多的损失函数，解决更多的问题。

## 描述
输入：训练集 $T=\{(x_1,y_1),..,(x_N,y_N)\},\; x_i \in R^n,\; y_i \in R$。

输出：提升树模型$f_M(x)$。

1. 初始化一棵回归树，最小化当前损失，$f_0(x)=\color{Red} \arg\underset{c}{\min}\sum_{i=1}^NL(y_i,c)$
2. 对 $m=1,2,...,m$，
    1. 对 $i=1,2,..,N$，**计算负梯度**，$r_{m,i}=-\frac{\partial L(y_i,{\color{Red} f_{m-1}(x_i)}))}{\partial {\color{Red} f_{m-1}(x_i)}}$
    2. 对 $r_{m,i}$ **拟合一棵回归树**，得到第m棵树的叶节点区域：$R_{m,j},\;j=1,2,..,J$
    3. 对 $j=1,2,..,J$，**估计叶结点区域的值**，$c_{m,j}={\color{Red} \arg\underset{c}{\min}}\sum_{x_i\in R_{m,j}}L(y_i,{\color{Blue} f_{m-1}(x_i)+c})$
    4. 更新当前模型，$f_m(x)=f_{m-1}(x)+\sum_{j=1}^J c_{m,j}{\color{Blue} I(x\in R_{m,j})}$
3. 得到最终的提升树模型，$f_M(x)=\sum_{i=1}^M\sum_{j=1}^Jc_{m,j}{\color{Blue} I(x\in R_{m,j})}$

说明：
- 第 1 步初始化，估计使损失函数最小的常数值，得到一棵只有一个根节点的树。对于分类，该常数值为训练数据中目标值的众数；对于标准回归，该常数值是训练数据中目标值的平均数。
- 第 2(i) 步**计算损失函数的负梯度**，将其作为残差的估计。对平方损失而言，负梯度就是残差；对于一般的损失函数，它是残差的近似。
- 第 2(ii) 步**拟合一棵回归树**，以负梯度为目标值。此时叶子结点保存了负梯度的近似值。
- 第 2(iii) 步利用线性搜索**估计叶节点区域的值，更新叶子结点**。对不同的损失函数，估计叶结点区域的值有不同的方法。在算法实现中，一般用学习率来代替线性搜索。
- 第 2(iv) 步更新当前模型。


## 如何做多分类
> [深入理解GBDT多分类算法](https://zhuanlan.zhihu.com/p/91652813?utm_source=wechat_session)

如果数据集有多个类别，需要对每个样本的类标签进行onehot处理。举个例子，假设数据集有3个类别，样本$(x,y)$属于第二个类别。onehot处理后，样本的类标签$y$变为$(0,1,0)^T$。此时，该样本分身为3个数据，分别是$(x,0), (x,1), (x,0)$。

如果数据集有K个类别，一个数据变为K个数据，数据量变为原来的K倍，需要对每个类标签的数据拟合一棵回归树。所以，对于多分类问题，每次需要拟合K棵回归树。


## 损失函数
介绍一下`sklearn`中GBDT使用的损失函数。==TODO==

给定一个样本$(x,y)$，$x \in R^d$是样本特征，$y$是样本的真实目标值。给定$\hat{y}$是当前模型对该样本的预测值。


### 分类
一、交叉熵损失函数

二分类交叉熵：
- 损失函数的形式为：$L(x) = -[y\ln{h(\hat{y})}+ (1-y)\ln{(1-h(\hat{y}))}]=-[y\hat{y}+\ln{(1-h(\hat{y}))}]$，其中$y \in \{0,1\}$，$h(\hat{y})=1/(1+e^{-\hat{y}})$为sigmoid变换函数，其输出值可以认为是该样本属于正类的概率。
- 损失函数对当前模型的负梯度为：$-\frac{\partial{L}}{\partial{\hat{y}}}=y-h(\hat{y})$。

多分类交叉熵：
- 在多分类问题中，需要对目标值$y$进行onehot处理。假设有K个类别，$y$转化为K维向量$(y[1],y[2],...,y[K])^T \in R^K$，其中一个元素对应一个类别，而且只有一个元素为1，其余均为0。预测值$\hat{y} \in R^K$，其中每个元素$\hat{y}[k]$为对应类别`k`的预测值。
- 损失函数的形式为：$L(x)=-\sum_{j=1}^{K}y[j]\ln h(\hat{y}[j])$，其中$h(\hat{y}[j])=e^{\hat{y}[j]}{}/ \sum_{k=1}^{K}e^{\hat{y}[k]}$为softmax变换函数，其输出值可以认为是该样本属于类别`j`的概率。
- 损失函数对当前模型的负梯度为：$-\frac{\partial{L}}{\partial{\hat{y}}}=y-h(\hat{y}) \in R^K$。

二、指数损失函数

Adaboost算法使用该损失函数。该损失函数只适用于二分类。
- 损失函数的形式为：$L(x)=exp(-yh(\hat{y}))$，其中$y \in \{-1,1\}$。
- 损失函数对当前模型的负梯度为：

### 回归
一、最小平方误差损失函数

标准回归的损失函数。
- 损失函数的形式为：$L(x)=\frac{1}{2}(y-\hat{y})^2$。
- 损失函数对当前模型的负梯度为：$-\frac{\partial{L}}{\partial{\hat{y}}}=y-\hat{y}$。

二、最小绝对误差损失函数

- 损失函数的形式为：$L(x)=\frac{1}{2}|y-\hat{y}|$。
- 损失函数对当前模型的负梯度为：$-\frac{\partial{L}}{\partial{\hat{y}}}$。

三、huber损失函数

前面两者的组合，能够减少异常值的影响。

四、分位数损失函数。




# XGBoost
## 介绍
> [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)

XGBoost是陈天奇等人开发的高度优化的GBDT算法。

## 前提

## 推导
==TODO==

## 优点
优点包括：

1. XGBoost中的基学习器既可以使用CART回归树，也可以使用线性学习器
2. 加入了正则化项，包括叶子节点个数和叶子节点的得分值，防止过拟合
3. 不仅使用了一阶导数，还使用了二阶导数，损失更精确，而且还可以自定义损失函数
4. 在选择分裂特征时，使用了并行优化，而传统的GBDT会枚举每个特征
5. 考虑了训练数据存在稀疏值和缺失值的情况，可以为缺失值或特定的值指定分支的默认方向，提升了算法的效率
6. 支持数据抽样和列抽样，不仅能降低过拟合，还能减少计算量

## 参数
XGBoost的参数可以分为三类：普通参数，基学习器参数和学习目标参数。

1. 普通参数，控制总体的功能。
    - booster [default=gbtree]，基学习器的种类
2. 基学习器参数，控制单个学习器的属性。
    - eta/learning_rate [default=0.3]，学习率，一般为0.01-0.5之间
    - gamma [default=0]，结点分裂所需的最小损失减少阈值，损失小于该阈值则结点不进行分裂
    - max_depth [default=6]，树的最大深度，一般为3-10之间
    - min_child_weight [default=1]，叶子结点中的样本权重之和最小值。如果某次分裂产生一个叶子结点，其中的样本权重之和小于该值，则不会进行分裂
    - max_delta_step [default=0]，不太常用，默认值表示没有限制
    - subsample [default=1]，训练数据的抽样比例，可以防止过拟合
    - colsample_bytree [default=1]，列（特征）抽样的比例，防止过拟合
    - lambda [default=1]，L2正则化系数
    - alpha [default=0]，L1正则化系数
3. 学习目标参数，控制训练目标的表现。
    - objective [default=reg:squarederror]
        - reg:squarederror，回归问题，平方损失
        - reg:squaredlogerror，回归问题，平方对数损失
        - binary:logistic，二分类逻辑损失，输出概率
        - binary:logitraw，二分类逻辑损失，输出逻辑转换之前的分数
        - binary:hinge，二分类折页损失，预测为0或1，不能输出概率
        - multi:softmax，多分类softmax损失，需要设置num_class参数
        - multi:softprob，和前者相同，但是输出一个向量，表示每个样本属于每个类的概率
    - eval_metric[default according to objective]，验证数据的评估指标。回归默认rmse，分类一般为error，还包括rmse、mae、logloss、error、merror、mlogloss、auc。



# 评价指标

## 分类
### 分类准确率
分类准确（错误）率是分类正确（错误）的个数占所有样本的比例。

### 查准率，查全率，F1-score
||实际为正例|实际为负例|
|---|---|---|
|预测为正例（positive)|TP (True Positive)|FP (False Positive)|
|预测为负例（negative)|FN (False Negative)|TN (True Negative)|

- 查准率Precision：$P=TP/(TP+FP)$
- 查全率Recall：$R=TP/(TP+FN)$
- F1-score:$f1=2*P*R/(P+R)$

### ROC,AUC
ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。ROC曲线下的面积称为AUC（Area Under Curve），AUC可以用来衡量“二分类问题”机器学习算法性。

ROC曲线的横坐标为假阳性率（False Positive Rate，FPR），纵坐标为真阳性率（True Positive Rate，TPR），其中
$$
TPR=\frac{TP}{TP+FN}, \;\;FPR=\frac{FP}{FP+TN}.
$$

给定$m$个样本，其中$m^+$个正例和$m^-$个反例。根据学习器预测的概率对样例进行排序，最可能是正例的排在前面，最不可能是正例的排在最后面。然后把分类阈值设为最大，此时把所有样例预测为反例，此时TPR和FPR均为0。依次把阈值设置为每个样例的预测值，大于等于该阈值的预测为正例，计算TPR和FPR，同时对相邻的点进行连线。

从定义可知，AUC可通过对ROC曲线下各部分的面积求和而得。假设ROC曲线是由坐标$\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}$的点按序连接而成（$x_0=1, x_m=1$），则AUC可估算为
$$
AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1}).
$$

为什么使用ROC/AUC？ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。

## 回归
### 平均平方误差
MSE (mean squared error): $\frac{1}{m}\sum_{i=1}^{m}(y_i-\hat{y}_i)^2$，其中$y_i$为真实值，$\hat{y}_i$为预测值。

### 平均绝对误差
MAE (mean absolute error): $\frac{1}{m}\sum_{i=1}^{m}|y_i-\hat{y}_i|$，其中$y_i$为真实值，$\hat{y}_i$为预测值。




# 各种问题
## 泛化误差，偏差，方差，噪声
> 《机器学习》——周志华

泛化误差可分解为偏差、方差与噪声之和。

偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，既能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

**自己的话**：偏差用来描述算法的拟合能力，体现在算法的训练误差；方差用来描述算法的稳定性，体现在数据扰动后算法误差的变化。

## 欠拟合，过拟合
欠拟合，拟合训练数据不够好，导致模型性能较差。具体表现是模型在训练集和测试集上都有较高的误差。

过拟合，拟合训练数据太好，导致模型性能较差。具体表现是模型在训练上的训练误差较小，在测试集上的误差较大。

## 如何解决欠拟合，过拟合
解决欠拟合：
- 添加特征项。挖掘目前没有的特征。
- 添加多项式特征。对当前特征进行组合。
- 增加模型的复杂度。比如把一次模型换成二次模型。
- 减小正则化系数。

解决过拟合：
- 清理数据。数据不纯会导致过拟合。
- 增加训练数据。训练数据相对模型较少会导致过拟合。
- 降低模型复杂度。
- 增加正则化。
- 使用提前停止，减少迭代次数。
- 对于决策树，可以进行剪枝。
- 对于神经网络，可以使用dropout方法。

## 如何解决类别不平衡问题
主要解决方法有：

1. 扩大数据集，增加小类的数据样本
2. 对大类数据欠采样，减少大类的数据样本个数，使与小样本个数接近。缺点是欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。
    - 代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。
3. 对小类数据过采样，增加小类的数据样本个数。
    - 代表算法：SMOTE。通过对训练集中的小类数据进行插值来产生额外的小类样本数据。对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。
4. 使用新评价指标。准确率不适合用来评价不平衡的数据集。 
5. 选择新的算法。
6. 对数据的重要性进行加权。

# sklearn

sklearn是一个比较常用也非常好用的机器学习库。sklearn支持分类，回归，聚类和降维四大类机器学习算法，还包括特征提取，数据处理和模型评估等工具。sklearn的官方文档非常详细，代码的可读性和易用性非常强。

关于源码：

1. sklean中的分类、回归等模型都用了Mixin设计模式，每个模型单独设计一个类，让具体的类来继承，表示这是一个什么模型。每个Mixin类中还写了适当的的函数，比如分类就有一个默认的评分函数，使用的是准确率。
2. 决策树模型有很多参数，树的最大深度，叶子结点的最小样本个数，最大叶子节点个数。如果设置了最大叶子结点个数，那么决策树采用深度优先建树策略，否则就采用广度优先建树策略。
3. GBDT模型中的基学习器为CART回归树，不管是分类任务还是回归任务，所以GBDT实现了一个基类`BaseGradientBoosting`，让GBDT的分类器和回归器都继承这一个类。
