> https://www.ruanyifeng.com/blog/2019/11/fault-tolerance.html

# 介绍

高并发是指在同一个时间点，有很多用户同时的访问同一个系统。它经常会发生在有大活跃用户量，用户高聚集的业务场景中。为了支持高并发，需要通过一定的设计保证系统能够同时并发很多请求。

高可用（High Availability）通常来描述一个系统经过专门的设计，从而减少停工时间，而保持其服务的高度可用性。

另外，还有两个和高可用相似的词语：容错和灾备，下面稍微对比一下。

## 容错

[容错](https://en.wikipedia.org/wiki/Fault_tolerance)（Fault Tolerance）指的是， **发生故障时，系统还能继续运行。**

飞机有四个引擎，如果一个引擎坏了，剩下三个引擎，还能继续飞，这就是"容错"。同样的，汽车的一个轮子扎破了，剩下三个轮子，也还是勉强能行驶。

容错的目的是，发生故障时，系统的运行水平可能有所下降，但是依然可用，不会完全失败。

## 高可用

[高可用](https://en.wikipedia.org/wiki/High_availability)（High Availability）指的是， **系统能够比正常时间更久地保持一定的运行水平。**

汽车的备胎就是一个高可用的例子。如果没有备胎，轮胎坏了，车就开不久了。备胎延长了汽车行驶的可用时间。

注意，高可用不是指系统不中断（那是容错能力），而是指一旦中断能够快速恢复，即中断必须是短暂的。如果需要很长时间才能恢复可用性，就不叫高可用了。上面例子中，更换备胎就必须停车，但只要装上去，就能回到行驶状态。

## 灾备

> https://pdai.tech/md/arch/arch-y-backup.html

[灾备](https://en.wikipedia.org/wiki/Disaster_recovery)（Disaster Recovery，又称灾难恢复）指的是， **发生灾难时恢复业务的能力。**

飞机是你的 IT 基础设施，飞行员是你的业务，飞行员弹射装置就是灾备措施。一旦飞机即将坠毁，你的基础设施就要没了，灾备可以让你的业务幸存下来。

灾备的目的就是，保存系统的核心部分。一个好的灾备方案，就是从失败的基础设施中获取企业最宝贵的数据，然后在新的基础设施上恢复它们。注意，灾备不是为了挽救基础设置，而是为了挽救业务。

# 缓存

数据库每秒能接受的请求次数也是有限的（或者文件的读写也是有限的），如何能够有效利用有限的资源来提供尽可能大的吞吐量? 一个有效的办法就是引入缓存，打破标准流程，每个环节中请求可以从缓存中直接获取目标数据并返回，从而减少计算量，有效提升响应速度，让有限的资源服务更多的用户。

## 缓存介质

虽然从硬件介质上来看，无非就是内存和硬盘两种，但从技术上，可以分成内存、硬盘文件、数据库。

- 内存：将缓存存储于内存中是最快的选择，无需额外的I/O开销，但是内存的缺点是没有持久化落地物理磁盘，一旦应用异常break down而重新启动，数据很难或者无法复原。
- 硬盘：一般来说，很多缓存框架会结合使用内存和硬盘，在内存分配空间满了或是在异常的情况下，可以被动或主动的将内存空间数据持久化到硬盘中，达到释放空间或备份数据的目的。
- 数据库：前面有提到，增加缓存的策略的目的之一就是为了减少数据库的I/O压力。现在使用数据库做缓存介质是不是又回到了老问题上了? 其实，数据库也有很多种类型，像那些不支持SQL，只是简单的key-value存储结构的特殊数据库（如BerkeleyDB和Redis），响应速度和吞吐量都远远高于我们常用的关系型数据库等。

## 缓存应用和实现

缓存有各类特征，而且有不同介质的区别，那么实际工程中我们怎么去对缓存分类呢? 在目前的应用服务框架中，比较常见的是根据缓存与应用的藕合度，分为local cache（本地缓存）和remote cache（分布式缓存）：

- **本地缓存**：指的是在应用中的缓存组件，其最大的优点是应用和cache是在同一个进程内部，请求缓存非常快速，没有过多的网络开销等，在单应用不需要集群支持或者集群情况下各节点无需互相通知的场景下使用本地缓存较合适；同时，它的缺点也是应为缓存跟应用程序耦合，多个应用程序无法直接的共享缓存，各应用或集群的各节点都需要维护自己的单独缓存，对内存是一种浪费。
- **分布式缓存**：指的是与应用分离的缓存组件或服务，其最大的优点是自身就是一个独立的应用，与本地应用隔离，多个应用可直接的共享缓存。

目前各种类型的缓存都活跃在成千上万的应用服务中，还没有一种缓存方案可以解决一切的业务场景或数据类型，我们需要根据自身的特殊场景和背景，选择最适合的缓存方案。

## 高并发缓存问题

### 缓存一致性问题

当数据时效性要求很高时，需要保证缓存中的数据与数据库中的保持一致，而且需要保证缓存节点和副本中的数据也保持一致，不能出现差异现象。这就比较依赖缓存的过期和更新策略。一般会在数据发生更改的时，主动更新缓存中的数据或者移除对应的缓存。

### 缓存并发问题

缓存过期后将尝试从后端数据库获取数据，这是一个看似合理的流程。但是，在高并发场景下，有可能多个请求并发的去从数据库获取数据，对后端数据库造成极大的冲击，甚至导致 “雪崩”现象。此外，当某个缓存key在被更新时，同时也可能被大量请求在获取，这也会导致一致性的问题。那如何避免类似问题呢? 我们会想到类似“锁”的机制，在缓存更新或者过期的情况下，先尝试获取到锁，当更新或者从数据库获取完成后再释放锁，其他的请求只需要牺牲一定的等待时间，即可直接从缓存中继续获取数据。

### 缓存穿透问题

缓存穿透在有些地方也称为“击穿”。很多朋友对缓存穿透的理解是：由于缓存故障或者缓存过期导致大量请求穿透到后端数据库服务器，从而对数据库造成巨大冲击。

这其实是一种误解。真正的缓存穿透应该是这样的：在高并发场景下，如果某一个key被高并发访问，没有被命中，出于对容错性考虑，会尝试去从后端数据库中获取，从而导致了大量请求达到数据库，而当该key对应的数据本身就是空的情况下，这就导致数据库中并发的去执行了很多不必要的查询操作，从而导致巨大冲击和压力。

通过下面的几种常用方式来避免缓存传统问题：

- 缓存空对象：对查询结果为空的对象也进行缓存，如果是集合，可以缓存一个空的集合（非null），如果是缓存单个对象，可以通过字段标识来区分。这样避免请求穿透到后端数据库。同时，也需要保证缓存数据的时效性。这种方式实现起来成本较低，比较适合命中不高，但可能被频繁更新的数据。
- 单独过滤处理：对所有可能对应数据为空的key进行统一的存放，并在请求前做拦截，这样避免请求穿透到后端数据库。这种方式实现起来相对复杂，比较适合命中不高，但是更新不频繁的数据。

### 缓存抖动问题

缓存抖动可以看做是一种比“雪崩”更轻微的故障，但是也会在一段时间内对系统造成冲击和性能影响。一般是由于缓存节点故障导致。业内推荐的做法是通过一致性Hash算法来解决。这里不做过多阐述。

### 缓存雪崩问题

缓存雪崩就是指由于缓存的原因，导致大量请求到达后端数据库，从而导致数据库崩溃，整个系统崩溃，发生灾难。导致这种现象的原因有很多种，上面提到的“缓存并发”，“缓存穿透”，“缓存颠簸”等问题，其实都可能会导致缓存雪崩现象发生。这些问题也可能会被恶意攻击者所利用。还有一种情况，例如某个时间点内，系统预加载的缓存周期性集中失效了，也可能会导致雪崩。为了避免这种周期性失效，可以通过设置不同的过期时间，来错开缓存过期，从而避免缓存集中失效。

从应用架构角度，我们可以通过限流、降级、熔断等手段来降低影响，也可以通过多级缓存来避免这种灾难。

此外，从整个研发体系流程的角度，应该加强压力测试，尽量模拟真实场景，尽早的暴露问题从而防范。

## 合理利用缓存

不合理使用缓存非但不能提高系统的性能，还会成为系统的累赘，甚至风险。

### 频繁修改的数据

如果缓存中保存的是频繁修改的数据，就会出现数据写入缓存后，应用还来不及读取缓存，数据就已经失效，徒增系统负担。一般来说，数据的读写比在2：1（写入一次缓存，在数据更新前至少读取两次）以上，缓存才有意义。

### 没有热点的访问

如果应用系统访问数据没有热点，不遵循二八定律，那么缓存就没有意义。

### 数据不一致与脏读

一般会对缓存的数据设置失效时间，一旦超过失效时间，就要从数据库中重新加载。因此要容忍一定时间的数据不一致，如卖家已经编辑了商品属性，但是需要过一段时间才能被买家看到。还有一种策略是数据更新立即更新缓存，不过这也会带来更多系统开销和事务一致性问题。

### 缓存可用性

缓存会承担大部分数据库访问压力，数据库已经习惯了有缓存的日子，所以当缓存服务崩溃时，数据库会因为完全不能承受如此大压力而宕机，导致网站不可用。这种情况被称作缓存雪崩，发生这种故障，甚至不能简单地重启缓存服务器和数据库服务器来恢复。

实践中，有的网站通过缓存热备份等手段提高缓存可用性：当某台缓存服务器宕机时，将缓存访问切换到热备服务器上。但这种设计有违缓存的初衷，缓存根本就不应该当做一个可靠的数据源来使用。

通过分布式缓存服务器集群，将缓存数据分布到集群多台服务器上可在一定程度上改善缓存的可用性。当一台缓存服务器宕机时，只有部分缓存数据丢失，重新从数据库加载这部分数据不会产生很大的影响。

### 缓存预热（warm up）

缓存中存放的是热点数据，热点数据又是缓存系统利用LRU（最近最久未用算法）对不断访问的数据筛选淘汰出来，这个过程需要花费较长的时间。新系统的缓存系统如果没有任何数据，在重建缓存数据的过程中，系统的性能和数据库负载都不太好，那么最好在缓存系统启动时就把热点数据加载好，这个缓存预加载手段叫缓存预热。对于一些元数据如城市地名列表、类目信息，可以在启动时加载数据库中全部数据到缓存进行预热。

### 避免缓存穿透

如果因为不恰当的业务、或者恶意攻击持续高并发地请求某个不存在的数据，由于缓存没有保存该数据，所有的请求都会落到数据库上，会对数据库造成压力，甚至崩溃。一个简单的对策是将不存在的数据也缓存起来(其value为null)。

# 限流

> https://juejin.cn/post/7056068978862456846 https://juejin.cn/post/6967742960540581918 https://github.com/jiaxwu/limiter

限流，也称流量控制，是指在系统面临高并发的情况下，**限制新的请求对系统的访问**，从而**保证系统的稳定性**。

限流是项目中经常需要使用到的一种工具，一般用于限制用户的请求的频率，也可以避免瞬间流量过大导致系统崩溃，或者稳定消息处理速率。

限流可能会导致部分用户请求处理不及时或者被拒，这就影响了用户体验。所以一般需要在系统稳定和用户体验之间**平衡**一下。

下面介绍几种常见的限流算法。

## 固定窗口

固定窗口算法根据窗口时间间隔开启一个`新的窗口`，在每一个窗口时间内，最多可以通过一定数量的请求。

该算法主要是会存在`临界问题`，如果流量都集中在两个窗口的交界处，那么突发流量会是设置上限的两倍。

另外，当流量达到上限，后面的请求都会被拒绝。

### 实现原理

固定窗口算法需要维护一个计数器，将单位时间段当做一个窗口，计数器记录这个窗口接收请求的次数。

- 当次数少于限流阀值，就允许访问，并且计数器+1。
- 当次数大于限流阀值，就拒绝访问。
- 当前的时间窗口过去之后，计数器清零。

假设单位时间是1秒，限流阀值为3。在单位时间1秒内，每来一个请求，计数器就加1，如果计数器累加的次数超过限流阀值3，后续的请求全部拒绝。等到1s结束后，计数器清0，重新开始计数。如下图：

### Go语言实现

代码如下：

```Go
package limiter

import (
   "sync"
   "time"
)

// FixedWindowLimiter 固定窗口限流器
type FixedWindowLimiter struct {
   limit    int           // 窗口请求上限
   window   time.Duration // 窗口时间大小
   counter  int           // 计数器
   lastTime time.Time     // 上一次请求的时间
   mutex    sync.Mutex    // 避免并发问题
}

func NewFixedWindowLimiter(limit int, window time.Duration) *FixedWindowLimiter {
   return &FixedWindowLimiter{
      limit:    limit,
      window:   window,
      lastTime: time.Now(),
   }
}

func (l *FixedWindowLimiter) TryAcquire() bool {
   l.mutex.Lock()
   defer l.mutex.Unlock()
   // 获取当前时间
   now := time.Now()
   // 如果当前窗口失效，计数器清0，开启新的窗口
   if now.Sub(l.lastTime) > l.window {
      l.counter = 0
      l.lastTime = now
   }
   // 若到达窗口请求上限，请求失败
   if l.counter >= l.limit {
      return false
   }
   // 若没到窗口请求上限，计数器+1，请求成功
   l.counter++
   return true
}
```

### Redis实现

如果服务是分布式部署的，那么在设置限流时就需要考虑是限制整个服务的流量，还是限制每一个节点的流量。如果是前者，那么需要使用分布式限流，比如使用Redis实现固定窗口算法。

具体来说，设置一个string类型的key，以窗口为key的过期时间，以窗口内的请求数量为值。

如果这个key不存在，说明key过期了。

- 如果这个key的value大于等于限流阈值，返回false。
- 如果这个key的value小于限流阈值，设置value加1。

代码如下：

```Go
package redis

import (
  "context"
  "errors"
  "github.com/go-redis/redis/v8"
  "time"
)

const fixedWindowLimiterTryAcquireRedisScript = `
-- ARGV[1]: 窗口时间大小
-- ARGV[2]: 窗口请求上限
local window = tonumber(ARGV[1])
local limit = tonumber(ARGV[2])
-- 获取原始值
local counter = tonumber(redis.call("get", KEYS[1]))
if counter == nil then 
  counter = 0
end
-- 若到达窗口请求上限，请求失败
if counter >= limit then
  return 0
end
-- 窗口值+1
redis.call("incr", KEYS[1])
if counter == 0 then
    redis.call("pexpire", KEYS[1], window)
end
return 1
`

// FixedWindowLimiter 固定窗口限流器
type FixedWindowLimiter struct {
  limit  int           // 窗口请求上限
  window int           // 窗口时间大小
  client *redis.Client // Redis客户端
  script *redis.Script // TryAcquire脚本
}

func NewFixedWindowLimiter(client *redis.Client, limit int, window time.Duration) (*FixedWindowLimiter, error) {
  // redis过期时间精度最大到毫秒，因此窗口必须能被毫秒整除
  if window%time.Millisecond != 0 {
    return nil, errors.New("the window uint must not be less than millisecond")
  }

  return &FixedWindowLimiter{
    limit:  limit,
    window: int(window / time.Millisecond),
    client: client,
    script: redis.NewScript(fixedWindowLimiterTryAcquireRedisScript),
  }, nil
}

func (l *FixedWindowLimiter) TryAcquire(ctx context.Context, resource string) error {
  success, err := l.script.Run(ctx, l.client, []string{resource}, l.window, l.limit).Bool()
  if err != nil {
    return err
  }
  // 若到达窗口请求上限，请求失败
  if !success {
    return ErrAcquireFailed
  }
  return nil
}
```

## 滑动窗口

滑动窗口类似于固定窗口，它只是把大窗口切分成多个小窗口，分别记录每个小窗口内接口的访问次数，并且根据时间滑动删除过期的小窗口。

滑动窗口限流算法可以解决固定窗口临界值的问题。但是，和固定窗口相同，当流量达到上限，后面的请求都会被拒绝。

### 实现原理

滑动窗口算法如下图所示：

![img](https://raw.githubusercontent.com/shengchaohua/my-images/main/images/202311262132550.png)

假设单位时间还是1s，滑动窗口算法把它划分为5个小周期，也就是滑动窗口（单位时间）被划分为5个小格子。每格表示0.2s。每过0.2s，时间窗口就会往右滑动一格。然后呢，每个小周期，都有自己独立的计数器，如果请求是0.83s到达的，0.8~1.0s对应的计数器就会加1。

我们来看下滑动窗口是如何解决临界问题的？

假设我们1s内的限流阀值还是5个请求，0.8~1.0s内（比如0.9s的时候）来了5个请求，落在黄色格子里。时间过了1.0s这个点之后，又来5个请求，落在紫色格子里。如果是固定窗口算法，是不会被限流的，但是滑动窗口的话，每过一个小周期，它会右移一个小格。过了1.0s这个点后，会右移一小格，当前的单位时间段是0.2~1.2s，这个区域的请求已经超过限定的5了，已触发限流啦，实际上，紫色格子的请求都被拒绝啦。 TIPS: 当滑动窗口的格子周期划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。

### Go语言实现

```Go
package limiter

import (
   "errors"
   "sync"
   "time"
)

// SlidingWindowLimiter 滑动窗口限流器
type SlidingWindowLimiter struct {
   limit        int           // 窗口请求上限
   window       int64         // 窗口时间大小
   smallWindow  int64         // 小窗口时间大小
   smallWindows int64         // 小窗口数量
   counters     map[int64]int // 小窗口计数器
   mutex        sync.Mutex    // 避免并发问题
}

// NewSlidingWindowLimiter 创建滑动窗口限流器
func NewSlidingWindowLimiter(limit int, window, smallWindow time.Duration) (*SlidingWindowLimiter, error) {
   // 窗口时间必须能够被小窗口时间整除
   if window%smallWindow != 0 {
      return nil, errors.New("window cannot be split by integers")
   }

   return &SlidingWindowLimiter{
      limit:        limit,
      window:       int64(window),
      smallWindow:  int64(smallWindow),
      smallWindows: int64(window / smallWindow),
      counters:     make(map[int64]int),
   }, nil
}

func (l *SlidingWindowLimiter) TryAcquire() bool {
   l.mutex.Lock()
   defer l.mutex.Unlock()

   // 获取当前小窗口值
   currentSmallWindow := time.Now().UnixNano() / l.smallWindow * l.smallWindow
   // 获取起始小窗口值
   startSmallWindow := currentSmallWindow - l.smallWindow*(l.smallWindows-1)

   // 计算当前窗口的请求总数
   var count int
   for smallWindow, counter := range l.counters {
      if smallWindow < startSmallWindow {
         delete(l.counters, smallWindow)
      } else {
         count += counter
      }
   }

   // 若到达窗口请求上限，请求失败
   if count >= l.limit {
      return false
   }
   // 若没到窗口请求上限，当前小窗口计数器+1，请求成功
   l.counters[currentSmallWindow]++
   return true
}
```

## 漏桶算法

漏桶是模拟一个漏水的桶，请求相当于往桶里倒水，处理请求的速度相当于水漏出的速度。

漏桶算法主要用于`请求处理速率较为稳定的服务`，需要使用生产者消费者模式把请求放到一个队列里，让消费者以一个较为稳定的速率处理。

漏桶算法面对限流，就更加的柔性，不存在直接的粗暴拒绝。

### 实现原理

漏桶算法的原理很简单，可以认为就是**注水漏水**的过程。漏桶中以任意速率流入水，以固定的速率流出水。当水超过桶的容量时，会被溢出，也就是被丢弃。因为桶容量是不变的，保证了整体的速率。

如下图所示：

解释：

- 流入的水滴，可以看作是访问系统的请求，这个流入速率是不确定的。
- 桶的容量一般表示系统所能处理的请求数。
- 如果桶的容量满了，就达到限流的阀值，就会丢弃水滴（拒绝请求）
- 流出的水滴，是恒定过滤的，对应服务按照固定的速率处理请求。

### Go语言实现

```Go
package limiter

import (
   "sync"
   "time"
)

// LeakyBucketLimiter 漏桶限流器
type LeakyBucketLimiter struct {
   peakLevel       int        // 最高水位
   currentLevel    int        // 当前水位
   currentVelocity int        // 水流速度/秒
   lastTime        time.Time  // 上次放水时间
   mutex           sync.Mutex // 避免并发问题
}

func NewLeakyBucketLimiter(peakLevel, currentVelocity int) *LeakyBucketLimiter {
   return &LeakyBucketLimiter{
      peakLevel:       peakLevel,
      currentVelocity: currentVelocity,
      lastTime:        time.Now(),
   }
}

func (l *LeakyBucketLimiter) TryAcquire() bool {
   l.mutex.Lock()
   defer l.mutex.Unlock()

   // 尝试放水
   now := time.Now()
   // 距离上次放水的时间
   interval := now.Sub(l.lastTime)
   if interval >= time.Second {
      // 当前水位-距离上次放水的时间(秒)*水流速度
      l.currentLevel = maxInt(0, l.currentLevel-int(interval/time.Second)*l.currentVelocity)
      l.lastTime = now
   }

   // 若到达最高水位，请求失败
   if l.currentLevel >= l.peakLevel {
      return false
   }
   // 若没有到达最高水位，当前水位+1，请求成功
   l.currentLevel++
   return true
}

func maxInt(a, b int) int {
   if a > b {
      return a
   }
   return b
}
```

## 令牌桶

与漏桶算法相反，令牌桶会不断地把令牌添加到桶里，而请求会从桶中获取令牌，只有拥有令牌地请求才能被接受。

令牌桶算法`允许一定地突发流量通过`，因为我们可以在桶中可以提前添加一些令牌。

如果令牌发放的策略正确，这个系统即不会被拖垮，也能提高机器的利用率。Guava的RateLimiter限流组件，就是基于**令牌桶算法**实现的。

### 实现原理

令牌桶的原理包括：

- 有一个令牌管理员，根据限流大小，定速往令牌桶里放令牌。
- 如果令牌数量满了，超过令牌桶容量的限制，那就丢弃。
- 系统在接受到一个用户请求时，都会先去令牌桶要一个令牌。如果拿到令牌，那么就处理这个请求的业务逻辑；
- 如果拿不到令牌，就直接拒绝这个请求。

如下图所示：

![img](https://secure2.wostatic.cn/static/mNp2G7nE4nngG9EhReT53p/image.png)

### Go语言实现

```Go
package limiter

import (
   "sync"
   "time"
)

// TokenBucketLimiter 令牌桶限流器
type TokenBucketLimiter struct {
   capacity      int        // 容量
   currentTokens int        // 令牌数量
   rate          int        // 发放令牌速率/秒
   lastTime      time.Time  // 上次发放令牌时间
   mutex         sync.Mutex // 避免并发问题
}

func NewTokenBucketLimiter(capacity, rate int) *TokenBucketLimiter {
   return &TokenBucketLimiter{
      capacity: capacity,
      rate:     rate,
      lastTime: time.Now(),
   }
}

func (l *TokenBucketLimiter) TryAcquire() bool {
   l.mutex.Lock()
   defer l.mutex.Unlock()

   // 尝试发放令牌
   now := time.Now()
   // 距离上次发放令牌的时间
   interval := now.Sub(l.lastTime)
   if interval >= time.Second {
      // 当前令牌数量+距离上次发放令牌的时间(秒)*发放令牌速率
      l.currentTokens = minInt(l.capacity, l.currentTokens+int(interval/time.Second)*l.rate)
      l.lastTime = now
   }

   // 如果没有令牌，请求失败
   if l.currentTokens == 0 {
      return false
   }
   // 如果有令牌，当前令牌-1，请求成功
   l.currentTokens--
   return true
}

func minInt(a, b int) int {
   if a < b {
      return a
   }
   return b
}
```

## 滑动日志

滑动日志与滑动窗口算法类似，但是滑动日志主要用于`多级限流`的场景，比如短信验证码1分钟1次，1小时10次，1天20次这种业务。

滑动日志算法的流程与滑动窗口相同，但是可以指定多个策略，同时在请求失败的时候，需要通知调用方是被哪个策略所拦截。

```Go
package limiter

import (
   "errors"
   "fmt"
   "sort"
   "sync"
   "time"
)

// ViolationStrategyError 违背策略错误
type ViolationStrategyError struct {
   Limit  int           // 窗口请求上限
   Window time.Duration // 窗口时间大小
}

func (e *ViolationStrategyError) Error() string {
   return fmt.Sprintf("violation strategy that limit = %d and window = %d", e.Limit, e.Window)
}

// SlidingLogLimiterStrategy 滑动日志限流器的策略
type SlidingLogLimiterStrategy struct {
   limit        int   // 窗口请求上限
   window       int64 // 窗口时间大小
   smallWindows int64 // 小窗口数量
}

func NewSlidingLogLimiterStrategy(limit int, window time.Duration) *SlidingLogLimiterStrategy {
   return &SlidingLogLimiterStrategy{
      limit:  limit,
      window: int64(window),
   }
}

// SlidingLogLimiter 滑动日志限流器
type SlidingLogLimiter struct {
   strategies  []*SlidingLogLimiterStrategy // 滑动日志限流器策略列表
   smallWindow int64                        // 小窗口时间大小
   counters    map[int64]int                // 小窗口计数器
   mutex       sync.Mutex                   // 避免并发问题
}

func NewSlidingLogLimiter(smallWindow time.Duration, strategies ...*SlidingLogLimiterStrategy) (*SlidingLogLimiter, error) {
   // 复制策略避免被修改
   strategies = append(make([]*SlidingLogLimiterStrategy, 0, len(strategies)), strategies...)

   // 不能不设置策略
   if len(strategies) == 0 {
      return nil, errors.New("must be set strategies")
   }

   // 排序策略，窗口时间大的排前面，相同窗口上限大的排前面
   sort.Slice(strategies, func(i, j int) bool {
      a, b := strategies[i], strategies[j]
      if a.window == b.window {
         return a.limit > b.limit
      }
      return a.window > b.window
   })
   fmt.Println(strategies[0], strategies[1])

   for i, strategy := range strategies {
      // 随着窗口时间变小，窗口上限也应该变小
      if i > 0 {
         if strategy.limit >= strategies[i-1].limit {
            return nil, errors.New("the smaller window should be the smaller limit")
         }
      }
      // 窗口时间必须能够被小窗口时间整除
      if strategy.window%int64(smallWindow) != 0 {
         return nil, errors.New("window cannot be split by integers")
      }
      strategy.smallWindows = strategy.window / int64(smallWindow)
   }

   return &SlidingLogLimiter{
      strategies:  strategies,
      smallWindow: int64(smallWindow),
      counters:    make(map[int64]int),
   }, nil
}

func (l *SlidingLogLimiter) TryAcquire() error {
   l.mutex.Lock()
   defer l.mutex.Unlock()

   // 获取当前小窗口值
   currentSmallWindow := time.Now().UnixNano() / l.smallWindow * l.smallWindow
   // 获取每个策略的起始小窗口值
   startSmallWindows := make([]int64, len(l.strategies))
   for i, strategy := range l.strategies {
      startSmallWindows[i] = currentSmallWindow - l.smallWindow*(strategy.smallWindows-1)
   }

   // 计算每个策略当前窗口的请求总数
   counts := make([]int, len(l.strategies))
   for smallWindow, counter := range l.counters {
      if smallWindow < startSmallWindows[0] {
         delete(l.counters, smallWindow)
         continue
      }
      for i := range l.strategies {
         if smallWindow >= startSmallWindows[i] {
            counts[i] += counter
         }
      }
   }

   // 若到达对应策略窗口请求上限，请求失败，返回违背的策略
   for i, strategy := range l.strategies {
      if counts[i] >= strategy.limit {
         return &ViolationStrategyError{
            Limit:  strategy.limit,
            Window: time.Duration(strategy.window),
         }
      }
   }

   // 若没到窗口请求上限，当前小窗口计数器+1，请求成功
   l.counters[currentSmallWindow]++
   return nil
}
```

## 总结

如果需要一个简单高效的算法，可以使用固定窗口，但是它可能产生两倍的突发流量。

可以通过滑动窗口避免突发流量问题，但是窗口可能会掐断流量一段时间。

如果需要更平滑的流量，可以使用漏桶算法搭配生产者消费者模式。

如果能够处理一定的突发流量，可以使用令牌桶算法。

遇到多级限流的场景，滑动日志会更加适合。

# 熔断

> https://cloud.tencent.com/developer/article/1815254 https://blog.csdn.net/claram/article/details/104725233

熔断是指调用方访问服务时通过断路器做代理进行访问，断路器会持续观察服务返回的成功、失败的状态，当失败超过设置的阈值时断路器打开，请求就不能真正地访问到服务了。断路器相当于一个开关，打开后可以阻止流量通过。比如保险丝，当电流过大时，就会熔断，从而避免元器件损坏。

## 服务雪崩

服务雪崩是指一个服务的下游服务发生故障，导致上游服务也无法正常工作，整条调用链路的服务都变得不可用。

假定服务A依赖服务B，服务B依赖服务C，当服务B和服务C处于正常状态，整个调用是健康的，服务A可以得到服务B的正常响应。

如果服务A的流量波动很大，那么在服务A的流量非常大时，就算服务A能扛得住请求，服务B和服务C未必能扛得住这突发的请求。

当服务C出现故障时，比如响应缓慢或者响应超时，如果服务B继续请求服务C，那么服务B的响应时间也会增加，进而导致上游的服务A也不可用。

在服务的依赖调用中，被调用方出现故障时，出于自我保护的目的，调用方会主动停止调用，并根据业务需要进行相应处理。这种调用方主动停止调用的行为可以称为熔断。

## 断路器

服务熔断通常使用断路器模式。

断路器有3种状态：

- CLOSED：默认状态。断路器观察到请求失败比例没有达到阈值，断路器认为被代理服务状态良好。
- OPEN：断路器观察到请求失败比例已经达到阈值，断路器认为被代理服务故障，打开开关，请求不再到达被代理的服务，而是快速失败。
- HALF OPEN：断路器打开后，为了能自动恢复对被代理服务的访问，会切换到半开放状态，去尝试请求被代理服务以查看服务是否已经故障恢复。如果成功，会转成CLOSED状态，否则转到OPEN状态。

断路器的状态切换图如下：

![img](https://secure2.wostatic.cn/static/7i3eHis5vuf8Y9ELDPJyGu/image.png)

使用断路器有一些注意事项：

- 针对不同的异常，定义不同的熔断后处理逻辑。
- 设置熔断的时长，服务恢复后关闭断路器。
- 记录请求失败日志，供监控使用。
- 补偿接口，断路器可以提供补偿接口让运维人员手工关闭。

使用场景：

- 服务故障或者升级时，让客户端快速失败。
- 失败处理逻辑容易定义。
- 响应耗时较长，客户端设置的`read timeout`会比较长，防止客户端大量重试请求导致的连接、线程资源不能释放。

举个例子，

![img](https://secure2.wostatic.cn/static/tauCsUditEgdwZuQyBEFGG/image.png)

为什么要熔断？假定服务A依赖服务B，当服务B处于正常状态，整个调用是健康的，服务A可以得到服务B的正常响应。当服务B出现故障时，比如响应缓慢或者响应超时，如果服务A继续请求服务B，那么服务A的响应时间也会增加，进而导致服务A响应缓慢。如果服务A不进行熔断处理，服务B的故障会传导至服务A，最终导致服务A也不可用。

# 降级

服务降级是通过开关配置将某些不重要的业务功能屏蔽掉，在服务器资源固定的前提下，提高服务的处理能力。在大促场景中经常会对某些服务进行降级处理，大促结束之后再进行复原，比如电商中的商品评论。

相比限流和熔断来说，服务降级是站在系统全局的视角来考虑的。服务降级有很多种方式，比如开关降级、限流降级、熔断降级。

服务降级的使用场景包括：

- 服务处理异常，把异常信息直接反馈给客户端，不再走其他逻辑。
- 服务处理异常，把请求缓存下来，给客户端返回一个中间态，事后再重试缓存的请求。
- 监控系统检测到突增流量，为了避免非核心业务功能耗费系统资源，关闭这些非核心功能。
- 数据库请求压力大，可以考虑返回缓存中的数据。
- 对于耗时的写操作，可以改为异步写。
- 暂时关闭跑批任务，以节省系统资源。

## **总结**

限流、熔断和降级是系统容错和高可用的重要设计模式，从一定意义上讲限流和熔断也是一种服务降级的手段。

熔断和服务降级主要是针对非核心业务功能，而核心业务如果流程超过预估的峰值，就需要进行限流。

对于限流，选择合理的限流算法很重要，令牌桶算法优势很明显，也是使用最多的限流算法。

在系统设计的时候，这些模式需要配合业务量的预估、性能测试的数据进行相应阈值的配置，而这些阈值最好保存在[配置中心](https://cloud.tencent.com/product/tse?from=10680)，方便实时修改。

# 负载均衡

## 简介

面对大量用户访问、高并发请求，海量数据，可以使用高性能的服务器、大型数据库，存储设备，高性能Web服务器，采用高效率的编程语言比如(Go,Scala)等，当单机容量达到极限时，我们需要考虑业务拆分和分布式部署，来解决大型网站访问量大，并发量高，海量数据的问题。

从单机网站到分布式网站，很重要的区别是业务拆分和分布式部署，将应用拆分后，部署到不同的机器上，实现大规模分布式系统。分布式和业务拆分解决了，从集中到分布的问题，但是每个部署的独立业务还存在单点的问题和访问统一入口问题，为解决单点故障，我们可以采取冗余的方式。将相同的应用部署到多台机器上。解决访问统一入口问题，我们可以在集群前面增加负载均衡设备，实现流量分发。

负载均衡（Load Balance），意思是将负载（工作任务，访问请求）进行平衡、分摊到多个操作单元（服务器，组件）上进行执行。是解决高性能，单点故障（高可用），扩展性（水平伸缩）的终极解决方案。

### 负载均衡原理

系统的扩展可分为纵向（垂直）扩展和横向（水平）扩展。

纵向扩展，是从单机的角度通过增加硬件处理能力，比如CPU处理能力，内存容量，磁盘等方面，实现服务器处理能力的提升，不能满足大型分布式系统（网站），大流量，高并发，海量数据的问题。因此需要采用横向扩展的方式。

横向扩展是指通过添加机器来满足大型网站服务的处理能力。比如：一台机器不能满足，则增加两台或者多台机器，共同承担访问压力。

### 负载均衡的方式

**应用集群**：将同一应用部署到多台机器上，组成处理集群，接收负载均衡设备分发的请求，进行处理，并返回相应数据。

**负载均衡设备**：将用户访问的请求，根据负载均衡算法，分发到集群中的一台处理服务器。（一种把网络请求分散到一个服务器集群中的可用服务器上去的设备）

### 负载均衡的作用

1.解决并发压力，提高应用处理性能（增加吞吐量，加强网络处理能力）；

2.提供故障转移，实现高可用；

3.通过添加或减少服务器数量，提供网站伸缩性（扩展性）；

4.安全防护；（负载均衡设备上做一些过滤，黑白名单等处理）。

## 负载均衡分类

根据实现技术不同，可分为DNS负载均衡，HTTP负载均衡，IP负载均衡，链路层负载均衡等。

### DNS负载均衡

最早的负载均衡技术，利用域名解析实现负载均衡，在DNS服务器，配置多个A记录，这些A记录对应的服务器构成集群。大型网站总是部分使用DNS解析，作为第一级负载均衡。

**优点**

- 使用简单：负载均衡工作，交给DNS服务器处理，省掉了负载均衡服务器维护的麻烦
- 提高性能：可以支持基于地址的域名解析，解析成距离用户最近的服务器地址，可以加快访问速度，改善性能；

**缺点**

- **可用性差**：DNS解析是多级解析，新增/修改DNS后，解析时间较长；解析过程中，用户访问网站将失败；
- **扩展性低**：DNS负载均衡的控制权在域名商那里，无法对其做更多的改善和扩展；
- **维护性差**：也不能反映服务器的当前运行状态；支持的算法少；不能区分服务器的差异（不能根据系统与服务的状态来判断负载）

建议：将DNS作为第一级负载均衡，A记录对应着内部负载均衡的IP地址，通过内部负载均衡将请求分发到真实的Web服务器上。一般用于互联网公司，复杂的业务系统不合适使用。

### IP负载均衡

在网络层通过修改请求目标地址进行负载均衡。

用户请求数据包，到达负载均衡服务器后，负载均衡服务器在操作系统内核进程获取网络数据包，根据负载均衡算法得到一台真实服务器地址，然后将请求目的地址修改为，获得的真实ip地址，不需要经过用户进程处理。

真实服务器处理完成后，响应数据包回到负载均衡服务器，负载均衡服务器，再将数据包源地址修改为自身的ip地址，发送给用户浏览器。

### 链路层负载均衡

在通信协议的数据链路层修改mac地址，进行负载均衡。

数据分发时，不修改ip地址，指修改目标mac地址，配置真实物理服务器集群所有机器虚拟ip和负载均衡服务器ip地址一致，达到不修改数据包的源地址和目标地址，进行数据分发的目的。

实际处理服务器ip和数据请求目的ip一致，不需要经过负载均衡服务器进行地址转换，可将响应数据包直接返回给用户浏览器，避免负载均衡服务器网卡带宽成为瓶颈。

### 混合型负载均衡

由于多个服务器群内硬件设备、各自的规模、提供的服务等的差异，可以考虑给每个服务器群采用最合适的负载均衡方式，然后又在这多个服务器群间再一次负载均衡或群集起来以一个整体向外界提供服务（即把这多个服务器群当做一个新的服务器群），从而达到最佳的性能。将这种方式称之为混合型负载均衡。

## 常见的负载均衡服务器

常用的有四层负载均衡和七层负载均衡，四层的负载均衡是基于IP和端口实现的，七层的负载均衡是在四层的基础上，基于URL等信息实现。

### 四层负载均衡

LVS：重量级软件，本身不支持正则表达式，部署起来比较麻烦，但是性能高，应用范围广，一般的大型互联网公司都有用到。

HAProxy：轻量级软件，支持的负载均衡策略非常多，较灵活。

Nginx：轻量级软件，支持的协议少（HTTP、HTTPS和Email协议），对于Session支持不友好。

七层负载均衡

HAProxy：全面支持七层代理，灵活性高，支持Session会话保持。

Nginx：可以针对HTTP应用进行分流，正则规则灵活，支持高并发，部署简单。

Apache：性能较差，一般不考虑。

MySQL Proxy：官方的数据库中间件，可以实现读写分离，负载均衡等功能，但是对分表分库支持不完善（可选替代品：Atlas，Cobar，TDDL）。

## 常见的负载均衡算法

常见的负载均衡算法包含:

- 轮询法(Round Robin)
- 加权轮询法(Weight Round Robin)
- 平滑加权轮询法(Smooth Weight Round Robin)
- 随机法(Random)
- 加权随机法(Weight Random)
- 源地址哈希法(Hash)
- 最小连接数法(Least Connections)

### 随机法

随机法（Random）通过通过随机算法，在后端服务器的列表中随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。

### 加权随机法

不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。

加权随机法（Weighted Random）会根据后端机器的配置、系统的负载分配不同的权重，配置高、负载低的机器配置更高的权重。与此同时，权重大的服务器被选中的概率更大。

### 轮询法

轮询法（Round Robin）将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。

### 加权轮询法

和加权随机法类似，加权轮询法（Weighted Round Robin）给配置高、负载低的机器配置更高的权重，给配置低、负载高的机器，给其分配较低的权重。

加权轮询将请求分配按照权重顺序分配到各台机器。

### 源地址哈希法

源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。源地址哈希法有一个优点，同一IP地址的客户端，当后端服务器列表不变时，每次会访问同一台后端服务器，可以解决session的问题。

### 最小连接数法

最小连接数法（Least Connections）比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。

# **灾难恢复**

> https://zh.wikipedia.org/zh-cn/灾难恢复 https://pdai.tech/md/arch/arch-y-backup.html

灾难恢复（Disaster recovery，也称灾备），指自然或人为灾害后，重新启用信息系统的数据、硬件及软体设备，恢复正常商业运作的过程。灾难恢复规划是涵盖面更广的业务连续规划的一部分，其核心即对企业或机构的灾难性风险做出评估、防范，特别是对关键性业务资料、流程予以及时记录、备份、保护。

有两个和灾难恢复非常相似的概念：容灾和备份。

- **容灾**是为了在遭遇灾害时能保证信息系统能正常运行，帮助企业实现业务连续性的目标。
- **备份**是为了应对灾难来临时造成的数据丢失问题，平时做好数据等关键资料的备份。

灾难恢复包括容灾、备份、以及灾难恢复计划等，使用频率更高。

### 容灾分类

从其对系统的保护程度来分，可以将容灾系统分为:

- **数据级容灾**是指通过建立异地容灾中心，做数据的远程备份，在灾难发生之后要确保原有的数据不会丢失或者遭到破坏，但在数据级容灾这个级别，发生灾难时应用是会中断的。在数据级容灾方式下，所建立的异地容灾中心可以简单地把它理解成一个远程的数据备份中心。数据级容灾的恢复时间比较长，但是相比其他容灾级别来讲它的费用比较低，而且构建实施也相对简单。
- **应用级容灾**是在数据级容灾的基础之上，在备份站点同样构建一套相同的应用系统，通过同步或异步复制技术，这样可以保证关键应用在允许的时间范围内恢复运行，尽可能减少灾难带来的损失，让用户基本感受不到灾难的发生，这样就使系统所提供的服务是完整的、可靠的和安全的。应用级容灾生产中心和异地灾备中心之间的数据传输是采用异类的广域网传输方式；同时应用级容灾系统需要通过更多的软件来实现，可以使多种应用在灾难发生时可以进行快速切换，确保业务的连续性。
- **业务级容灾**是全业务的灾备，除了必要的IT相关技术，还要求具备全部的基础设施。其大部分内容是非IT系统（如电话、办公地点等），当大灾难发生后，原有的办公场所都会受到破坏，除了数据和应用的恢复，更需要一个备份的工作场所能够正常的开展业务。

## MySQL灾备

## Redis灾备